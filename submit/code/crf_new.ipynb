{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGkwOKpU5ovd",
        "outputId": "2a2f6805-3bf3-42e7-b70f-d3f19ccc46e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.11.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from scipy.special import logsumexp\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import json\n",
        "import regex as re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TAGS =  pickle.load(open( \"tags.pickle\", \"rb\" ))\n",
        "TAGS.remove(\"O\") \n",
        "NUM_TAGS = len(TAGS)"
      ],
      "metadata": {
        "id": "Y6vDaclU571Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag2id = {}\n",
        "for id,label in enumerate(TAGS):\n",
        "    tag2id[label] = id \n",
        "\n",
        "def label2id(labels):\n",
        "    ret = []\n",
        "    prev_label = \"\"\n",
        "    for label in labels:\n",
        "        if label == \"O\":\n",
        "            ret.append(str(2*NUM_TAGS))\n",
        "        elif label == prev_label:\n",
        "            l = [tag2id[t]+ NUM_TAGS for t in label]\n",
        "            for x in l:\n",
        "                ret.append(str(x))\n",
        "        else:\n",
        "            l =[tag2id[t] for t in label]\n",
        "            for x in l:\n",
        "                ret.append(str(x))\n",
        "        prev_label = label\n",
        "    return ret "
      ],
      "metadata": {
        "id": "3sHEy-hz6RtS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label(label_id):\n",
        "    if label_id == (2*NUM_TAGS):\n",
        "        return \"O\"\n",
        "    elif label_id >= NUM_TAGS:\n",
        "        return [TAGS[label_id-NUM_TAGS]]\n",
        "    else:\n",
        "        return TAGS[label_id]\n",
        "\n",
        "def id2label(labels):\n",
        "    ret = []\n",
        "    for label in labels:\n",
        "        l = [get_label(x) for x in label]\n",
        "        if len(l) == 1 and l[0] == \"O\":\n",
        "            l = \"O\"\n",
        "        ret.append(l)\n",
        "    return ret \n",
        "\n",
        "def clean_text(sent):\n",
        "    ret_sent= []\n",
        "    for txt in sent:\n",
        "      fil_txt = re.sub('[^A-Za-z0-9]+', '', str(txt))\n",
        "      if len(fil_txt) == 0:\n",
        "        fil_txt  = txt [0]\n",
        "      ret_sent.append(fil_txt)\n",
        "    assert(len(ret_sent) == len(sent))\n",
        "    return ret_sent"
      ],
      "metadata": {
        "id": "YDGQj_ar6gEV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_STATES = 2*NUM_TAGS+1\n",
        "NUM_FEATURES = 0\n",
        "NUM_INST = 0\n",
        "\n",
        "# load train data\n",
        "with open('dev.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "a = [d[\"sent\"] for d in data]\n",
        "set_ = set()\n",
        "for idx,s in enumerate(a):\n",
        "    for t in s:\n",
        "        if len(t)<1:\n",
        "            set_.add(idx)\n",
        "data = [data[i] for i in range(len(data)) if i not in set_]\n",
        "\n",
        "unique_word_set = set()\n",
        "for x in data:\n",
        "  for w in x[\"sent\"]:\n",
        "    unique_word_set.add(w)\n",
        "words_to_id = {}\n",
        "for idx, w in enumerate(unique_word_set):\n",
        "  words_to_id[w] = idx\n",
        "\n",
        "NUM_FEATURES = len(unique_word_set)\n",
        "NUM_INST = len(data)\n",
        "\n",
        "print(NUM_STATES, NUM_FEATURES, NUM_INST)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z3P4caL6jP-",
        "outputId": "bc604607-b446-4afc-bf72-4bd58e2142a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "227 42540 9956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_bool_vec(y_id):\n",
        "    y_bool = np.zeros(2*NUM_TAGS+1, bool)\n",
        "    num_labels = len(y_id)\n",
        "    for id in y_id:\n",
        "          y_bool[int(id)] = 1\n",
        "    return y_bool"
      ],
      "metadata": {
        "id": "lh4l9qq8_vu_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SentenceScanner(object):\n",
        "    def __init__(self, num_states, num_features, max_len, batch_size):\n",
        "        self._num_states = num_states\n",
        "        self._num_features = num_features\n",
        "        self._max_len = max_len\n",
        "        self._batch_size = batch_size\n",
        "        self._labeled_states = np.zeros((self._batch_size, self._max_len, self._num_states), dtype=np.int8)\n",
        "        self._labeled_emits = np.zeros((self._batch_size, self._max_len, self._num_features), dtype=np.int8)\n",
        "        # Also prepare a suffix mask to know where each sequence ended, a (B, T) tensor.\n",
        "        # This will let us ignore padded positions in the loss expression.\n",
        "        self._labeled_masks = np.zeros((self._batch_size, self._max_len), dtype=np.int8)\n",
        "        \n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def get_batch(self):\n",
        "        # Collect instances into ndarrays declared above.\n",
        "        num_sentence = 0\n",
        "        for data_point in data:\n",
        "            num_token = 0\n",
        "            labels = label2id(data_point[\"tags\"])\n",
        "            for idx, token in enumerate(data_point[\"sent\"]):\n",
        "                xid = words_to_id[token]\n",
        "                yid = to_bool_vec(labels[idx])\n",
        "                self._labeled_masks[num_sentence, num_token] = 1\n",
        "                self._labeled_emits[num_sentence, num_token, xid] = 1\n",
        "                for x in yid:\n",
        "                  if x:\n",
        "                    self._labeled_states[num_sentence, num_token, x] = 1\n",
        "                num_token += 1\n",
        "                if num_token >= self._max_len:\n",
        "                    break\n",
        "            num_sentence += 1\n",
        "            if num_sentence >= self._batch_size:\n",
        "                yield (self._labeled_masks, self._labeled_emits, self._labeled_states)\n",
        "                self._labeled_masks.fill(0)\n",
        "                self._labeled_emits.fill(0)\n",
        "                self._labeled_states.fill(0)\n",
        "                num_sentence = 0\n",
        "        if num_sentence > 0:\n",
        "            yield (self._labeled_masks, self._labeled_emits, self._labeled_states)\n",
        "\n",
        "\n",
        "# Clip at max sequence length T (starting with STATE_INIT).\n",
        "MAX_LEN = 105\n",
        "# M states, N instances (sentences), F features.\n",
        "# Convert training instances into [N, T, M] states tensor and [N, T, F] emission tensor.\n",
        "# We will generally not be able to hold all this in RAM, so we use batches.\n",
        "BATCH_SIZE = 51\n",
        "\n",
        "# TODO  Add code to shuffle sentences randomly and sample into train, dev, test folds.\n",
        "\n",
        "num_sentences = 0\n",
        "with tqdm(total=NUM_INST) as pbar:\n",
        "    ss = SentenceScanner(NUM_STATES, NUM_FEATURES, MAX_LEN, BATCH_SIZE)\n",
        "    for (_masks, _emits, _states) in ss.get_batch():\n",
        "        num_sentences += _masks.shape[0]\n",
        "        pbar.update(BATCH_SIZE)\n",
        "print(num_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsTAjrCP7pWl",
        "outputId": "843876a4-fdb3-4da0-fc90-23f114b08ca0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "9996it [00:15, 656.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChainCRF(object):\n",
        "    \"\"\"Implements linear chain CRF.\"\"\"\n",
        "    def __init__(self, state_init, num_states, num_features, max_len, batch_size):\n",
        "        self._num_states = num_states\n",
        "        self._num_features = num_features\n",
        "        self._max_len = max_len\n",
        "        self._batch_size = batch_size\n",
        "        # Trainable transition weights.  Rows = current state, columns = previous state.\n",
        "        self._edgew = tf.Variable(tf.random.uniform([self._num_states, self._num_states], \n",
        "                                                     dtype=tf.float64, minval=-1., maxval=1.),\n",
        "                                   trainable=True, name=\"edgew\")   #  (M, P)\n",
        "        # Trainable emission weights.  For starters we will use only lexicalized features.\n",
        "        self._nodew = tf.Variable(tf.random.uniform([self._num_states, self._num_features],\n",
        "                                                     dtype=tf.float64, minval=-1., maxval=1.),\n",
        "                                   trainable=True, name=\"nodew\")   #  (M, F)\n",
        "        # Labeled instances.\n",
        "        # Features may not be 1-hot in general. 1-hot state rep may be wasteful.\n",
        "        self._masks = tf.compat.v1.placeholder(tf.float64, shape=(self._batch_size, self._max_len),\n",
        "                                     name=\"masks\")   #  (B, T)\n",
        "        self._emits = tf.compat.v1.placeholder(tf.float64, shape=(self._batch_size, self._max_len,\n",
        "                                                        self._num_features), name=\"emits\")  # (B, T, F)\n",
        "        self._states = tf.compat.v1.placeholder(tf.float64, shape=(self._batch_size, self._max_len,\n",
        "                                                         self._num_states), name=\"states\")  # (B, T, M)\n",
        "        self._pad_states_np = np.zeros((self._batch_size, 1, self._num_states))\n",
        "        self._pad_states_np[:,:, state_init] = 1\n",
        "        pad_states = tf.constant(self._pad_states_np, dtype=tf.float64)\n",
        "        self._prev_states = tf.concat([pad_states, self._states[:,:-1,:] ],\n",
        "                                      axis=1, name=\"prev_states\") # (B, T, P)\n",
        "        # P = M but we use a distinct symbol to highlight the distinction between previous and current states.\n",
        "        print(self._nodew)\n",
        "        print(self._edgew)\n",
        "        print(self._masks)\n",
        "        print(self._emits)\n",
        "        print(self._states)\n",
        "        print(self._prev_states)\n",
        "\n",
        "        # To look up w \\cdot \\varphi(x_t, m, p) for all instances in the batch, we need\n",
        "        # corresponding tensor wvarphi_t with shape (B, T, M, P).\n",
        "        # We want wvarphi_t[b, t, p, m] =  ( sum_f nodew[m, f] emits[b, t, f] ) + edgew[p, m]\n",
        "        # for all possible combinations of m, p in [M] \\times [P], not just the gold sequence.\n",
        "        # The first term results in shape (B, T, M) and the second term results in shape (M, P).\n",
        "        # These have to be expanded to shape (B, T, M, P).\n",
        "\n",
        "        var1 = tf.einsum(\"btf,mf->btm\", self._emits, self._nodew, name=\"var1\")  # .... (B, T, M)\n",
        "        print(var1)\n",
        "        var2 = tf.expand_dims(var1, axis=3, name=\"var2\")    #  .... (B, T, M, 1)\n",
        "        print(var2)\n",
        "        var3 = tf.tile(var2, [1, 1, 1, self._num_states], name=\"var3\")   # .... (B, T, M, P)\n",
        "        print(var3)\n",
        "\n",
        "        # edge_weights is (M, P)\n",
        "        var4 = tf.expand_dims(self._edgew, axis=0, name=\"var4\")   #  (1, M, P)\n",
        "        print(var4)\n",
        "        var5 = tf.tile(var4, [self._max_len, 1, 1], name=\"var5\")   #  (T, M, P)\n",
        "        print(var5)\n",
        "        var6 = tf.expand_dims(var5, axis=0, name=\"var6\")   #  (1, T, M, P)\n",
        "        print(var6)\n",
        "        var7 = tf.tile(var6, [self._batch_size, 1, 1, 1], name=\"var7\")   # ... (B, T, M, P)\n",
        "        print(var7)\n",
        "\n",
        "        self._wvarphi_t = tf.add(var3, var7, name=\"wvarphi_t\")    # .... (B, T, M, P)\n",
        "        print(self._wvarphi_t)\n",
        "        \n",
        "        # For given emissions and state labels, find score w \\cdot \\phi(x, y).\n",
        "        self._scores_t = tf.einsum(\"btmp,btp,btm->bt\", self._wvarphi_t,\n",
        "                                   self._prev_states, self._states, name=\"scores_t\")  #  (B,T)\n",
        "        print(self._scores_t)\n",
        "        self._scores = tf.reduce_sum(tf.multiply(self._scores_t, self._masks),\n",
        "                                     axis=1, name=\"scores\")    #  ... (B)\n",
        "        print(self._scores)\n",
        "        \n",
        "        # Alpha recurrence over time steps.\n",
        "        self._lalpha = tf.Variable(initial_value=np.zeros((self._batch_size, self._num_states)),\n",
        "                                  trainable=True, name=\"lalpha_0\")   # .... (B, M)\n",
        "        print(self._lalpha)\n",
        "        for t in range(self._max_len):\n",
        "            var8 = tf.tile(tf.expand_dims(self._lalpha, axis=1), [1, self._num_states, 1])  #  (B, M, P)\n",
        "            next_lalpha = tf.reduce_logsumexp(var8 + self._wvarphi_t[:,t,:,:],  # (B, M, P)\n",
        "                                              axis=2, name=\"lalpha_\"+str(t+1))\n",
        "            mask_t = tf.tile(tf.expand_dims(self._masks[:,t], axis=1), [1, self._num_states])\n",
        "            self._lalpha = tf.multiply(mask_t, next_lalpha) + tf.multiply(1.-mask_t, self._lalpha)\n",
        "        print(self._lalpha)\n",
        "        \n",
        "        # For given emissions, find log Z over all possible state label sequences.\n",
        "        self._logz = tf.reduce_logsumexp(self._lalpha, axis=1, name=\"logz\")   # ... (B)\n",
        "        print(self._logz)\n",
        "        # We have to maximize scores - logZ i.e. minimize logZ - score.\n",
        "        self._loss = tf.reduce_sum(self._logz - self._scores, name=\"loss\")    # ... (B)\n",
        "        print(self._loss)\n",
        "        adamopt = tf.optimizers.Adam(learning_rate=0.1)\n",
        "        self._tape = tf.GradientTape(persistent=True)\n",
        "        self._train_op = adamopt.minimize(self._loss, var_list=[self._nodew, self._edgew], tape=self._tape)\n",
        "\n",
        "        \n",
        "    def check_np_scores(self, sess, masks, emitss, statess):\n",
        "        \"\"\"\n",
        "        masks, emitss, statess are for a whole batch.\n",
        "        Calculates w \\cdot \\phi conventionally using numpy to check correctness.\n",
        "        \"\"\"\n",
        "        _nodew = sess.run(self._nodew)\n",
        "        _edgew = sess.run(self._edgew)\n",
        "        ans = np.zeros((self._batch_size))\n",
        "        for b in range(self._batch_size):\n",
        "            mask = masks[b,:]\n",
        "            emits = emitss[b,:,:]\n",
        "            states = statess[b,:,:]\n",
        "            prev_states = np.concatenate((self._pad_states_np[b,:,:], states[:-1,:]), axis=0)\n",
        "            potscore = 0\n",
        "            for t in range(self._max_len):\n",
        "                aemit = emits[t,:]\n",
        "                aprev_state = prev_states[t,:]\n",
        "                astate = statess[b,t,:]\n",
        "                nodepot = np.matmul(astate, np.matmul(_nodew, aemit))\n",
        "                edgepot = np.matmul(astate, np.matmul(_edgew, aprev_state))\n",
        "                potscore += (nodepot + edgepot)\n",
        "            ans[b] = potscore\n",
        "        return ans\n",
        "    \n",
        "    def check_tf_scores(self, sess, masks, emitss, statess):\n",
        "        tf_scores = sess.run(self._scores, feed_dict = {\n",
        "            self._masks: masks, self._emits: emitss, self._states: statess })\n",
        "        return tf_scores\n",
        "\n",
        "    def check_np_logzs(self, sess, masks, emitss, statess):\n",
        "        \"\"\"\n",
        "        Calculates log Z conventionally using numpy to check correctness.\n",
        "        \"\"\"\n",
        "        np_wvarphi_t = sess.run(self._wvarphi_t, feed_dict={\n",
        "            self._masks: masks, self._emits: emitss, self._states: statess})\n",
        "        #print(\"np_wvarphi_t\", np_wvarphi_t.shape)   # (B, T, M, P)\n",
        "        logzs = np.zeros((self._batch_size))\n",
        "        for b in range(self._batch_size):\n",
        "            np_lalpha = np.zeros((self._num_states))  # (P) or (M)\n",
        "            for t in range(self._max_len):\n",
        "                np_lalpha_next = np.zeros((self._num_states))  # (M)\n",
        "                for m in range(self._num_states):\n",
        "                    softsummand = np.zeros((self._num_states))  # (P)\n",
        "                    for p in range(self._num_states):\n",
        "                        softsummand[p] = np_wvarphi_t[b,t,m,p] + np_lalpha[p]\n",
        "                    np_lalpha_next[m] = logsumexp(softsummand)\n",
        "                np_lalpha = np_lalpha_next\n",
        "            logzs[b] = logsumexp(np_lalpha)\n",
        "        return logzs\n",
        "\n",
        "    def check_tf_logzs(self, sess, masks, emitss, statess):\n",
        "        tf_logzs = sess.run(self._logz, feed_dict={\n",
        "            self._masks: masks, self._emits: emitss, self._states: statess})\n",
        "        return tf_logzs\n",
        "\n",
        "    def do_train(self, sess, num_epochs=10):\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        # TODO Add code to load any partially trained model for warm-start.\n",
        "        chart_batches, chart_losses = list(), list()\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        #plt.ion()\n",
        "        fig.show()\n",
        "        fig.canvas.draw()\n",
        "        num_batches = 0\n",
        "        # TODO keep history of loss objectives\n",
        "        for _ in range(num_epochs):\n",
        "            with tqdm(total=num_sentences) as pbar:\n",
        "                ss = SentenceScanner(self._num_states, self._num_features, self._max_len, self._batch_size)\n",
        "                for (masks, emits, states) in ss.get_batch():\n",
        "                    num_batches += 1\n",
        "                    sess.run(self._train_op, feed_dict = { \n",
        "                        self._masks: masks, self._emits: emits, self._states: states })\n",
        "                    _logZ = sess.run(self._logz, feed_dict = { \n",
        "                        self._masks: masks, self._emits: emits, self._states: states })\n",
        "                    _scores = sess.run(self._scores, feed_dict = { \n",
        "                        self._masks: masks, self._emits: emits, self._states: states })\n",
        "                    _loss = np.sum(_logZ - _scores)\n",
        "                    assert _loss >= 0\n",
        "\n",
        "                    chart_batches.append(num_batches)\n",
        "                    chart_losses.append(_loss)\n",
        "                    ax.clear()\n",
        "                    ax.plot(chart_batches, chart_losses)\n",
        "                    fig.canvas.draw()\n",
        "                    pbar.update(self._batch_size)\n",
        "                    pbar.set_description(\"%10g\" % _loss)\n",
        "                    \n",
        "                    if np.min(_logZ - _scores) < 0:\n",
        "                        print(\"tf_logzs - tf_scores\", _logZ - _scores)\n",
        "                        np_scores = self.check_np_scores(sess, masks, emits, states)\n",
        "                        tf_scores = self.check_tf_scores(sess, masks, emits, states)\n",
        "                        print(\"np_scores - tf_scores\", np.linalg.norm(np_scores - tf_scores, ord=np.inf))\n",
        "                        np_logzs = ccrf.check_np_logzs(sess, masks, emits, states)\n",
        "                        tf_logzs = ccrf.check_tf_logzs(sess, masks, emits, states)\n",
        "                        print(\"np_logzs - tf_logzs\", np.linalg.norm(np_logzs - tf_logzs, ord=np.inf))\n",
        "                        return\n",
        "            # TODO Add code to decide on ending training, saving model checkpoints.\n",
        "            \n",
        "    def get_fold_performance():\n",
        "        \"\"\"TODO Add code to calculate best labels sequences for current model, compare with gold\n",
        "        sequences, and return a measure of performance.\"\"\"\n",
        "        pass"
      ],
      "metadata": {
        "id": "JfY4id3Q-ckf"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.disable_eager_execution()"
      ],
      "metadata": {
        "id": "NB4cHUYpFgK5"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STATE_INIT = NUM_STATES-1\n",
        "ccrf = ChainCRF(STATE_INIT, NUM_STATES, NUM_FEATURES, MAX_LEN, BATCH_SIZE)\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    ccrf.do_train(sess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "id": "0CRF-JUSDX5Z",
        "outputId": "0f9e28a3-0875-4831-c12f-ad09b6e78c0d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'nodew_8:0' shape=(227, 42540) dtype=float64>\n",
            "<tf.Variable 'edgew_8:0' shape=(227, 227) dtype=float64>\n",
            "Tensor(\"masks_8:0\", shape=(51, 105), dtype=float64)\n",
            "Tensor(\"emits_8:0\", shape=(51, 105, 42540), dtype=float64)\n",
            "Tensor(\"states_8:0\", shape=(51, 105, 227), dtype=float64)\n",
            "Tensor(\"prev_states_7:0\", shape=(51, 105, 227), dtype=float64)\n",
            "Tensor(\"var1_7/Einsum:0\", shape=(51, 105, 227), dtype=float64)\n",
            "Tensor(\"var2_7:0\", shape=(51, 105, 227, 1), dtype=float64)\n",
            "Tensor(\"var3_7:0\", shape=(51, 105, 227, 227), dtype=float64)\n",
            "Tensor(\"var4_7:0\", shape=(1, 227, 227), dtype=float64)\n",
            "Tensor(\"var5_7:0\", shape=(105, 227, 227), dtype=float64)\n",
            "Tensor(\"var6_7:0\", shape=(1, 105, 227, 227), dtype=float64)\n",
            "Tensor(\"var7_7:0\", shape=(51, 105, 227, 227), dtype=float64)\n",
            "Tensor(\"wvarphi_t_7:0\", shape=(51, 105, 227, 227), dtype=float64)\n",
            "Tensor(\"scores_t_7/Einsum_1:0\", shape=(51, 105), dtype=float64)\n",
            "Tensor(\"scores_7:0\", shape=(51,), dtype=float64)\n",
            "<tf.Variable 'lalpha_0_7:0' shape=(51, 227) dtype=float64>\n",
            "Tensor(\"add_1679:0\", shape=(51, 227), dtype=float64)\n",
            "Tensor(\"logz_7:0\", shape=(51,), dtype=float64)\n",
            "Tensor(\"loss_7:0\", shape=(), dtype=float64)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-ba24adcadc32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mSTATE_INIT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUM_STATES\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mccrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChainCRF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTATE_INIT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_STATES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_FEATURES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mccrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-8a3abac8f3bc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_init, num_states, num_features, max_len, batch_size)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0madamopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersistent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madamopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edgew\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[1;32m    524\u001b[0m           \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \"\"\"\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_experimental/optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    246\u001b[0m                 \u001b[0;34m\"`tape` is required when a `Tensor` loss is passed. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;34mf\"Received: loss={loss}, tape={tape}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: `tape` is required when a `Tensor` loss is passed. Received: loss=Tensor(\"loss_7:0\", shape=(), dtype=float64), tape=None."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HqvCSEaPEMo-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}