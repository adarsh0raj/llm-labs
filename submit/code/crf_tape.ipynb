{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGkwOKpU5ovd",
    "outputId": "2a2f6805-3bf3-42e7-b70f-d3f19ccc46e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 15:11:50.048394: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-15 15:11:50.831649: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/compat/lib.real:/opt/conda/lib/python3.8/site-packages/torch/lib:/opt/conda/lib/python3.8/site-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-15 15:11:50.831745: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/compat/lib.real:/opt/conda/lib/python3.8/site-packages/torch/lib:/opt/conda/lib/python3.8/site-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-03-15 15:11:50.831754: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import regex as re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[K     |██████████████████████████▋     | 489.9 MB 92.2 MB/s eta 0:00:022"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 588.3 MB 82.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.47.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from tensorflow) (59.5.0)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 108.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 116.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl (21.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.5 MB 115.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[K     |████████████████████████████████| 439 kB 88.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 94.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.22.4)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 97.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 89.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 102.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (4.3.0)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 63.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 101.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.3.7)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Installing collected packages: protobuf, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.9.1\n",
      "    Uninstalling tensorboard-2.9.1:\n",
      "      Successfully uninstalled tensorboard-2.9.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 22.6.0a0+319.g97422602b8 requires protobuf<3.21.0a0,>=3.20.1, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-23.3.3 gast-0.4.0 google-pasta-0.2.0 h5py-3.8.0 keras-2.11.0 libclang-15.0.6.1 opt-einsum-3.3.0 protobuf-3.19.6 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.2.0 wrapt-1.15.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y6vDaclU571Z"
   },
   "outputs": [],
   "source": [
    "TAGS =  pickle.load(open( \"tags.pickle\", \"rb\" ))\n",
    "TAGS.remove(\"O\") \n",
    "NUM_TAGS = len(TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3sHEy-hz6RtS"
   },
   "outputs": [],
   "source": [
    "tag2id = {}\n",
    "for id,label in enumerate(TAGS):\n",
    "    tag2id[label] = id \n",
    "\n",
    "def label2id(labels):\n",
    "    ret = []\n",
    "    prev_label = \"\"\n",
    "    for label in labels:\n",
    "        if label == \"O\":\n",
    "            ret.append(str(2*NUM_TAGS))\n",
    "        elif label == prev_label:\n",
    "            l = [tag2id[t]+ NUM_TAGS for t in label]\n",
    "            for x in l:\n",
    "                ret.append(str(x))\n",
    "        else:\n",
    "            l =[tag2id[t] for t in label]\n",
    "            for x in l:\n",
    "                ret.append(str(x))\n",
    "        prev_label = label\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YDGQj_ar6gEV"
   },
   "outputs": [],
   "source": [
    "def get_label(label_id):\n",
    "    if label_id == (2*NUM_TAGS):\n",
    "        return \"O\"\n",
    "    elif label_id >= NUM_TAGS:\n",
    "        return [TAGS[label_id-NUM_TAGS]]\n",
    "    else:\n",
    "        return TAGS[label_id]\n",
    "\n",
    "def id2label(labels):\n",
    "    ret = []\n",
    "    for label in labels:\n",
    "        l = [get_label(x) for x in label]\n",
    "        if len(l) == 1 and l[0] == \"O\":\n",
    "            l = \"O\"\n",
    "        ret.append(l)\n",
    "    return ret \n",
    "\n",
    "def clean_text(sent):\n",
    "    ret_sent= []\n",
    "    for txt in sent:\n",
    "      fil_txt = re.sub('[^A-Za-z0-9]+', '', str(txt))\n",
    "      if len(fil_txt) == 0:\n",
    "        fil_txt  = txt [0]\n",
    "      ret_sent.append(fil_txt)\n",
    "    assert(len(ret_sent) == len(sent))\n",
    "    return ret_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Z3P4caL6jP-",
    "outputId": "bc604607-b446-4afc-bf72-4bd58e2142a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227 42540 9956\n"
     ]
    }
   ],
   "source": [
    "NUM_STATES = 2*NUM_TAGS+1\n",
    "NUM_FEATURES = 0\n",
    "NUM_INST = 0\n",
    "\n",
    "# load train data\n",
    "with open('data/dev.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "a = [d[\"sent\"] for d in data]\n",
    "set_ = set()\n",
    "for idx,s in enumerate(a):\n",
    "    for t in s:\n",
    "        if len(t)<1:\n",
    "            set_.add(idx)\n",
    "data = [data[i] for i in range(len(data)) if i not in set_]\n",
    "\n",
    "unique_word_set = set()\n",
    "for x in data:\n",
    "  for w in x[\"sent\"]:\n",
    "    unique_word_set.add(w)\n",
    "words_to_id = {}\n",
    "for idx, w in enumerate(unique_word_set):\n",
    "  words_to_id[w] = idx\n",
    "\n",
    "NUM_FEATURES = len(unique_word_set)\n",
    "NUM_INST = len(data)\n",
    "\n",
    "print(NUM_STATES, NUM_FEATURES, NUM_INST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lh4l9qq8_vu_"
   },
   "outputs": [],
   "source": [
    "def to_bool_vec(y_id):\n",
    "    y_bool = np.zeros(2*NUM_TAGS+1, bool)\n",
    "    num_labels = len(y_id)\n",
    "    for id in y_id:\n",
    "          y_bool[int(id)] = 1\n",
    "    return y_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EsTAjrCP7pWl",
    "outputId": "843876a4-fdb3-4da0-fc90-23f114b08ca0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9960it [00:07, 1350.21it/s]                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SentenceScanner(object):\n",
    "    def __init__(self, num_states, num_features, max_len, batch_size):\n",
    "        self._num_states = num_states\n",
    "        self._num_features = num_features\n",
    "        self._max_len = max_len\n",
    "        self._batch_size = batch_size\n",
    "        self._labeled_states = np.zeros((self._batch_size, self._max_len, self._num_states), dtype=np.int8)\n",
    "        self._labeled_emits = np.zeros((self._batch_size, self._max_len, self._num_features), dtype=np.int8)\n",
    "        # Also prepare a suffix mask to know where each sequence ended, a (B, T) tensor.\n",
    "        # This will let us ignore padded positions in the loss expression.\n",
    "        self._labeled_masks = np.zeros((self._batch_size, self._max_len), dtype=np.int8)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def get_batch(self):\n",
    "        # Collect instances into ndarrays declared above.\n",
    "        num_sentence = 0\n",
    "        for data_point in data:\n",
    "            num_token = 0\n",
    "            labels = label2id(data_point[\"tags\"])\n",
    "            for idx, token in enumerate(data_point[\"sent\"]):\n",
    "                xid = words_to_id[token]\n",
    "                yid = to_bool_vec(labels[idx])\n",
    "                self._labeled_masks[num_sentence, num_token] = 1\n",
    "                self._labeled_emits[num_sentence, num_token, xid] = 1\n",
    "                for x in yid:\n",
    "                  if x:\n",
    "                    self._labeled_states[num_sentence, num_token, x] = 1\n",
    "                num_token += 1\n",
    "                if num_token >= self._max_len:\n",
    "                    break\n",
    "            num_sentence += 1\n",
    "            if num_sentence >= self._batch_size:\n",
    "                yield (self._labeled_masks, self._labeled_emits, self._labeled_states)\n",
    "                self._labeled_masks.fill(0)\n",
    "                self._labeled_emits.fill(0)\n",
    "                self._labeled_states.fill(0)\n",
    "                num_sentence = 0\n",
    "        if num_sentence > 0:\n",
    "            yield (self._labeled_masks, self._labeled_emits, self._labeled_states)\n",
    "\n",
    "\n",
    "# Clip at max sequence length T (starting with STATE_INIT).\n",
    "MAX_LEN = 105\n",
    "# M states, N instances (sentences), F features.\n",
    "# Convert training instances into [N, T, M] states tensor and [N, T, F] emission tensor.\n",
    "# We will generally not be able to hold all this in RAM, so we use batches.\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "# TODO  Add code to shuffle sentences randomly and sample into train, dev, test folds.\n",
    "\n",
    "num_sentences = 0\n",
    "with tqdm(total=NUM_INST) as pbar:\n",
    "    ss = SentenceScanner(NUM_STATES, NUM_FEATURES, MAX_LEN, BATCH_SIZE)\n",
    "    for (_masks, _emits, _states) in ss.get_batch():\n",
    "        num_sentences += _masks.shape[0]\n",
    "        pbar.update(BATCH_SIZE)\n",
    "print(num_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JfY4id3Q-ckf"
   },
   "outputs": [],
   "source": [
    "class ChainCRF(object):\n",
    "    \"\"\"Implements linear chain CRF.\"\"\"\n",
    "    def __init__(self, state_init, num_states, num_features, max_len, batch_size):\n",
    "        self._num_states = num_states\n",
    "        self._num_features = num_features\n",
    "        self._max_len = max_len\n",
    "        self._batch_size = batch_size\n",
    "        # Trainable transition weights.  Rows = current state, columns = previous state.\n",
    "        self._edgew = tf.Variable(tf.random.uniform([self._num_states, self._num_states], \n",
    "                                                     dtype=tf.float64, minval=-1., maxval=1.),\n",
    "                                   trainable=True, name=\"edgew\")   #  (M, P)\n",
    "        # Trainable emission weights.  For starters we will use only lexicalized features.\n",
    "        self._nodew = tf.Variable(tf.random.uniform([self._num_states, self._num_features],\n",
    "                                                     dtype=tf.float64, minval=-1., maxval=1.),\n",
    "                                   trainable=True, name=\"nodew\")   #  (M, F)\n",
    "        # Labeled instances.\n",
    "        with tf.GradientTape(persistent=True) as tp:\n",
    "\n",
    "            # Features may not be 1-hot in general. 1-hot state rep may be wasteful.\n",
    "            self._masks = tf.compat.v1.placeholder(tf.float64, shape=(self._batch_size, self._max_len),\n",
    "                                         name=\"masks\")   #  (B, T)\n",
    "            self._emits = tf.compat.v1.placeholder(tf.float64, shape=(self._batch_size, self._max_len,\n",
    "                                                            self._num_features), name=\"emits\")  # (B, T, F)\n",
    "            self._states = tf.compat.v1.placeholder(tf.float64, shape=(self._batch_size, self._max_len,\n",
    "                                                             self._num_states), name=\"states\")  # (B, T, M)\n",
    "            self._pad_states_np = np.zeros((self._batch_size, 1, self._num_states))\n",
    "            self._pad_states_np[:,:, state_init] = 1\n",
    "            pad_states = tf.constant(self._pad_states_np, dtype=tf.float64)\n",
    "            self._prev_states = tf.concat([pad_states, self._states[:,:-1,:] ],\n",
    "                                          axis=1, name=\"prev_states\") # (B, T, P)\n",
    "            # P = M but we use a distinct symbol to highlight the distinction between previous and current states.\n",
    "            print(self._nodew)\n",
    "            print(self._edgew)\n",
    "            print(self._masks)\n",
    "            print(self._emits)\n",
    "            print(self._states)\n",
    "            print(self._prev_states)\n",
    "\n",
    "        # To look up w \\cdot \\varphi(x_t, m, p) for all instances in the batch, we need\n",
    "        # corresponding tensor wvarphi_t with shape (B, T, M, P).\n",
    "        # We want wvarphi_t[b, t, p, m] =  ( sum_f nodew[m, f] emits[b, t, f] ) + edgew[p, m]\n",
    "        # for all possible combinations of m, p in [M] \\times [P], not just the gold sequence.\n",
    "        # The first term results in shape (B, T, M) and the second term results in shape (M, P).\n",
    "        # These have to be expanded to shape (B, T, M, P).\n",
    "\n",
    "            var1 = tf.einsum(\"btf,mf->btm\", self._emits, self._nodew, name=\"var1\")  # .... (B, T, M)\n",
    "            print(var1)\n",
    "            var2 = tf.expand_dims(var1, axis=3, name=\"var2\")    #  .... (B, T, M, 1)\n",
    "            print(var2)\n",
    "            var3 = tf.tile(var2, [1, 1, 1, self._num_states], name=\"var3\")   # .... (B, T, M, P)\n",
    "            print(var3)\n",
    "\n",
    "            # edge_weights is (M, P)\n",
    "            var4 = tf.expand_dims(self._edgew, axis=0, name=\"var4\")   #  (1, M, P)\n",
    "            print(var4)\n",
    "            var5 = tf.tile(var4, [self._max_len, 1, 1], name=\"var5\")   #  (T, M, P)\n",
    "            print(var5)\n",
    "            var6 = tf.expand_dims(var5, axis=0, name=\"var6\")   #  (1, T, M, P)\n",
    "            print(var6)\n",
    "            var7 = tf.tile(var6, [self._batch_size, 1, 1, 1], name=\"var7\")   # ... (B, T, M, P)\n",
    "            print(var7)\n",
    "\n",
    "            self._wvarphi_t = tf.add(var3, var7, name=\"wvarphi_t\")    # .... (B, T, M, P)\n",
    "            print(self._wvarphi_t)\n",
    "            adamopt = tf.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "\n",
    "            # For given emissions and state labels, find score w \\cdot \\phi(x, y).\n",
    "            self._scores_t = tf.einsum(\"btmp,btp,btm->bt\", self._wvarphi_t,\n",
    "                                       self._prev_states, self._states, name=\"scores_t\")  #  (B,T)\n",
    "            print(self._scores_t)\n",
    "            self._scores = tf.reduce_sum(tf.multiply(self._scores_t, self._masks),\n",
    "                                         axis=1, name=\"scores\")    #  ... (B)\n",
    "            print(self._scores)\n",
    "\n",
    "            # Alpha recurrence over time steps.\n",
    "            self._lalpha = tf.Variable(initial_value=np.zeros((self._batch_size, self._num_states)),\n",
    "                                      trainable=True, name=\"lalpha_0\")   # .... (B, M)\n",
    "            print(self._lalpha)\n",
    "            for t in range(self._max_len):\n",
    "                var8 = tf.tile(tf.expand_dims(self._lalpha, axis=1), [1, self._num_states, 1])  #  (B, M, P)\n",
    "                next_lalpha = tf.reduce_logsumexp(var8 + self._wvarphi_t[:,t,:,:],  # (B, M, P)\n",
    "                                                  axis=2, name=\"lalpha_\"+str(t+1))\n",
    "                mask_t = tf.tile(tf.expand_dims(self._masks[:,t], axis=1), [1, self._num_states])\n",
    "                self._lalpha = tf.multiply(mask_t, next_lalpha) + tf.multiply(1.-mask_t, self._lalpha)\n",
    "            print(self._lalpha)\n",
    "            self._logz = tf.reduce_logsumexp(self._lalpha, axis=1, name=\"logz\")   # ... (B)\n",
    "            print(self._logz)\n",
    "            self._loss = tf.reduce_sum(self._logz - self._scores, name=\"loss\")    # ... (B)\n",
    "            print(self._loss)\n",
    "            \n",
    "        grad = tp.gradient(self._loss,  [self._nodew, self._edgew])\n",
    "        print(\"Hi-----------\", grad)\n",
    "        self._train_op = adamopt.apply_gradients(zip(grad,[self._nodew, self._edgew]))\n",
    "  \n",
    "#         self._train_op = adamopt.minimize(self._loss, var_list=[self._nodew, self._edgew])\n",
    "\n",
    "        \n",
    "    def check_np_scores(self, sess, masks, emitss, statess):\n",
    "        \"\"\"\n",
    "        masks, emitss, statess are for a whole batch.\n",
    "        Calculates w \\cdot \\phi conventionally using numpy to check correctness.\n",
    "        \"\"\"\n",
    "        _nodew = sess.run(self._nodew)\n",
    "        _edgew = sess.run(self._edgew)\n",
    "        ans = np.zeros((self._batch_size))\n",
    "        for b in range(self._batch_size):\n",
    "            mask = masks[b,:]\n",
    "            emits = emitss[b,:,:]\n",
    "            states = statess[b,:,:]\n",
    "            prev_states = np.concatenate((self._pad_states_np[b,:,:], states[:-1,:]), axis=0)\n",
    "            potscore = 0\n",
    "            for t in range(self._max_len):\n",
    "                aemit = emits[t,:]\n",
    "                aprev_state = prev_states[t,:]\n",
    "                astate = statess[b,t,:]\n",
    "                nodepot = np.matmul(astate, np.matmul(_nodew, aemit))\n",
    "                edgepot = np.matmul(astate, np.matmul(_edgew, aprev_state))\n",
    "                potscore += (nodepot + edgepot)\n",
    "            ans[b] = potscore\n",
    "        return ans\n",
    "    \n",
    "    def check_tf_scores(self, sess, masks, emitss, statess):\n",
    "        tf_scores = sess.run(self._scores, feed_dict = {\n",
    "            self._masks: masks, self._emits: emitss, self._states: statess })\n",
    "        return tf_scores\n",
    "\n",
    "    def check_np_logzs(self, sess, masks, emitss, statess):\n",
    "        \"\"\"\n",
    "        Calculates log Z conventionally using numpy to check correctness.\n",
    "        \"\"\"\n",
    "        np_wvarphi_t = sess.run(self._wvarphi_t, feed_dict={\n",
    "            self._masks: masks, self._emits: emitss, self._states: statess})\n",
    "        #print(\"np_wvarphi_t\", np_wvarphi_t.shape)   # (B, T, M, P)\n",
    "        logzs = np.zeros((self._batch_size))\n",
    "        for b in range(self._batch_size):\n",
    "            np_lalpha = np.zeros((self._num_states))  # (P) or (M)\n",
    "            for t in range(self._max_len):\n",
    "                np_lalpha_next = np.zeros((self._num_states))  # (M)\n",
    "                for m in range(self._num_states):\n",
    "                    softsummand = np.zeros((self._num_states))  # (P)\n",
    "                    for p in range(self._num_states):\n",
    "                        softsummand[p] = np_wvarphi_t[b,t,m,p] + np_lalpha[p]\n",
    "                    np_lalpha_next[m] = logsumexp(softsummand)\n",
    "                np_lalpha = np_lalpha_next\n",
    "            logzs[b] = logsumexp(np_lalpha)\n",
    "        return logzs\n",
    "\n",
    "    def check_tf_logzs(self, sess, masks, emitss, statess):\n",
    "        tf_logzs = sess.run(self._logz, feed_dict={\n",
    "            self._masks: masks, self._emits: emitss, self._states: statess})\n",
    "        return tf_logzs\n",
    "\n",
    "    def do_train(self, sess, num_epochs=10):\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        # TODO Add code to load any partially trained model for warm-start.\n",
    "        chart_batches, chart_losses = list(), list()\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        #plt.ion()\n",
    "        fig.show()\n",
    "        fig.canvas.draw()\n",
    "        num_batches = 0\n",
    "        # TODO keep history of loss objectives\n",
    "        for _ in range(num_epochs):\n",
    "            with tqdm(total=num_sentences) as pbar:\n",
    "                ss = SentenceScanner(self._num_states, self._num_features, self._max_len, self._batch_size)\n",
    "                for (masks, emits, states) in ss.get_batch():\n",
    "                    num_batches += 1\n",
    "                    sess.run(self._train_op, feed_dict = { \n",
    "                        self._masks: masks, self._emits: emits, self._states: states })\n",
    "                    _logZ = sess.run(self._logz, feed_dict = { \n",
    "                        self._masks: masks, self._emits: emits, self._states: states })\n",
    "                    _scores = sess.run(self._scores, feed_dict = { \n",
    "                        self._masks: masks, self._emits: emits, self._states: states })\n",
    "                    _loss = np.sum(_logZ - _scores)\n",
    "                    assert _loss >= 0\n",
    "\n",
    "                    chart_batches.append(num_batches)\n",
    "                    chart_losses.append(_loss)\n",
    "                    ax.clear()\n",
    "                    ax.plot(chart_batches, chart_losses)\n",
    "                    fig.canvas.draw()\n",
    "                    pbar.update(self._batch_size)\n",
    "                    pbar.set_description(\"%10g\" % _loss)\n",
    "                    \n",
    "                    if np.min(_logZ - _scores) < 0:\n",
    "                        print(\"tf_logzs - tf_scores\", _logZ - _scores)\n",
    "                        np_scores = self.check_np_scores(sess, masks, emits, states)\n",
    "                        tf_scores = self.check_tf_scores(sess, masks, emits, states)\n",
    "                        print(\"np_scores - tf_scores\", np.linalg.norm(np_scores - tf_scores, ord=np.inf))\n",
    "                        np_logzs = ccrf.check_np_logzs(sess, masks, emits, states)\n",
    "                        tf_logzs = ccrf.check_tf_logzs(sess, masks, emits, states)\n",
    "                        print(\"np_logzs - tf_logzs\", np.linalg.norm(np_logzs - tf_logzs, ord=np.inf))\n",
    "                        return\n",
    "            # TODO Add code to decide on ending training, saving model checkpoints.\n",
    "            \n",
    "    def get_fold_performance():\n",
    "        \"\"\"TODO Add code to calculate best labels sequences for current model, compare with gold\n",
    "        sequences, and return a measure of performance.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NB4cHUYpFgK5"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "0CRF-JUSDX5Z",
    "outputId": "0f9e28a3-0875-4831-c12f-ad09b6e78c0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'nodew_2:0' shape=(227, 42540) dtype=float64>\n",
      "<tf.Variable 'edgew_2:0' shape=(227, 227) dtype=float64>\n",
      "Tensor(\"masks_2:0\", shape=(5, 105), dtype=float64)\n",
      "Tensor(\"emits_2:0\", shape=(5, 105, 42540), dtype=float64)\n",
      "Tensor(\"states_2:0\", shape=(5, 105, 227), dtype=float64)\n",
      "Tensor(\"prev_states_2:0\", shape=(5, 105, 227), dtype=float64)\n",
      "Tensor(\"var1_2/Einsum:0\", shape=(5, 105, 227), dtype=float64)\n",
      "Tensor(\"var2_2:0\", shape=(5, 105, 227, 1), dtype=float64)\n",
      "Tensor(\"var3_2:0\", shape=(5, 105, 227, 227), dtype=float64)\n",
      "Tensor(\"var4_2:0\", shape=(1, 227, 227), dtype=float64)\n",
      "Tensor(\"var5_2:0\", shape=(105, 227, 227), dtype=float64)\n",
      "Tensor(\"var6_2:0\", shape=(1, 105, 227, 227), dtype=float64)\n",
      "Tensor(\"var7_2:0\", shape=(5, 105, 227, 227), dtype=float64)\n",
      "Tensor(\"wvarphi_t_2:0\", shape=(5, 105, 227, 227), dtype=float64)\n",
      "Tensor(\"scores_t_2/Einsum_1:0\", shape=(5, 105), dtype=float64)\n",
      "Tensor(\"scores_2:0\", shape=(5,), dtype=float64)\n",
      "<tf.Variable 'lalpha_0_2:0' shape=(5, 227) dtype=float64>\n",
      "Tensor(\"add_629:0\", shape=(5, 227), dtype=float64)\n",
      "Tensor(\"logz_2:0\", shape=(5,), dtype=float64)\n",
      "Tensor(\"loss_2:0\", shape=(), dtype=float64)\n",
      "Hi----------- [<tf.Tensor 'gradient_tape/var1_2/Einsum_1:0' shape=(227, 42540) dtype=float64>, <tf.Tensor 'gradient_tape/Reshape_977:0' shape=(227, 227) dtype=float64>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 15:15:23.300880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2640 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:87:00.0, compute capability: 8.0\n",
      "2023-03-15 15:15:23.303115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 47596 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:90:00.0, compute capability: 8.0\n",
      "2023-03-15 15:15:23.305561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 159 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:b7:00.0, compute capability: 8.0\n",
      "2023-03-15 15:15:23.308325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 47740 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:bd:00.0, compute capability: 8.0\n",
      "2023-03-15 15:15:24.134238: W tensorflow/c/c_api.cc:291] Operation '{name:'Adam_2/v/edgew_2/Assign' id:21479 op device:{requested: '', assigned: ''} def:{{{node Adam_2/v/edgew_2/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_DOUBLE, validate_shape=false](Adam_2/v/edgew_2, Adam_2/zeros_3)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "   18754.4:   0%|                             | 10/9960 [00:02<30:37,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_logzs - tf_scores [-4798.63540469 14305.32800279  2287.09273289  5820.69823679\n",
      "  1139.90930275]\n",
      "np_scores - tf_scores 12249.141766391003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   18754.4:   0%|                           | 10/9960 [00:15<4:20:23,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_logzs - tf_logzs 502.19377089196666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "STATE_INIT = NUM_STATES-1\n",
    "ccrf = ChainCRF(STATE_INIT, NUM_STATES, NUM_FEATURES, MAX_LEN, BATCH_SIZE)\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    ccrf.do_train(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqvCSEaPEMo-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
